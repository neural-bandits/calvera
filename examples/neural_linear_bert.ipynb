{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Any\n",
    "\n",
    "import lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import BertModel, DataCollatorForTokenClassification\n",
    "\n",
    "from calvera.bandits import NeuralLinearBandit\n",
    "from calvera.benchmark.datasets import ImdbMovieReviews\n",
    "from calvera.benchmark import BanditBenchmarkEnvironment, BertWrapper\n",
    "from calvera.utils import ListDataBuffer, AllDataBufferStrategy\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformers_collate(batch: Any, data_collator: DataCollatorForTokenClassification) -> Any:\n",
    "    \"\"\"Custom collate function for the DataLoader.\n",
    "\n",
    "    Args:\n",
    "        batch: The batch to collate.\n",
    "        data_collator: The data collator to use.\n",
    "\n",
    "    Returns:\n",
    "        The collated batch.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for item in batch:\n",
    "        inputs = item[0]\n",
    "        example = {\n",
    "            \"input_ids\": inputs[0],\n",
    "            \"attention_mask\": inputs[1],\n",
    "            \"token_type_ids\": inputs[2],\n",
    "        }\n",
    "        examples.append(example)\n",
    "\n",
    "    # Let the data collator process the list of individual examples.\n",
    "    context = data_collator(examples)\n",
    "    input_ids = context[\"input_ids\"]\n",
    "    attention_mask = context[\"attention_mask\"]\n",
    "    token_type_ids = context[\"token_type_ids\"]\n",
    "\n",
    "    if len(batch[0]) == 2:\n",
    "        realized_rewards = torch.stack([item[1] for item in batch])\n",
    "        return (input_ids, attention_mask, token_type_ids), realized_rewards\n",
    "\n",
    "    embedded_actions = None if batch[0][1] is None else torch.stack([item[1] for item in batch])\n",
    "    realized_rewards = torch.stack([item[2] for item in batch])\n",
    "    chosen_actions = None if batch[0][3] is None else torch.stack([item[3] for item in batch])\n",
    "\n",
    "    return (input_ids, attention_mask, token_type_ids), embedded_actions, realized_rewards, chosen_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the StatLog dataset\n",
    "dataset = ImdbMovieReviews()\n",
    "print(f\"Dataset context size: {dataset.context_size}\")\n",
    "print(f\"Dataset sample count: {len(dataset)}\")\n",
    "\n",
    "# Create data loader for a subset of the data\n",
    "collate_fn = partial(transformers_collate, data_collator=dataset.get_data_collator())\n",
    "train_loader = DataLoader(Subset(dataset, range(10000)), batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Set up the environment\n",
    "accelerator = \"cpu\"\n",
    "env = BanditBenchmarkEnvironment(train_loader, device=accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network and bandit\n",
    "network = BertWrapper(BertModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\", output_hidden_states=True).eval())\n",
    "\n",
    "buffer = ListDataBuffer(\n",
    "    buffer_strategy=AllDataBufferStrategy(),\n",
    "    max_size=1024,\n",
    ")\n",
    "\n",
    "bandit_module = NeuralLinearBandit(\n",
    "    network=network,\n",
    "    buffer=buffer,\n",
    "    n_embedding_size=128,\n",
    "    contextualization_after_network=True,  # <------- Very Important\n",
    "    n_arms=2,  # <-----------------------------------\n",
    "    initial_train_steps=128,\n",
    "    min_samples_required_for_training=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "rewards = np.array([])\n",
    "regrets = np.array([])\n",
    "progress = tqdm(iter(env), total=len(env))\n",
    "\n",
    "for contextualized_actions in progress:\n",
    "    # 1. Select actions\n",
    "    chosen_actions, _ = bandit_module.forward(contextualized_actions)\n",
    "\n",
    "    # 2. Create a trainer for this step\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=1,\n",
    "        enable_progress_bar=False,\n",
    "        enable_model_summary=False,\n",
    "        accelerator=accelerator,\n",
    "    )\n",
    "\n",
    "    # 3. Get feedback from environment\n",
    "    chosen_contextualized_actions, realized_rewards = env.get_feedback(chosen_actions)\n",
    "    batch_regret = env.compute_regret(chosen_actions)\n",
    "\n",
    "    # 4. Track metrics\n",
    "    rewards = np.append(rewards, realized_rewards.cpu().numpy())\n",
    "    regrets = np.append(regrets, batch_regret.cpu().numpy())\n",
    "    progress.set_postfix(\n",
    "        {\"reward\": realized_rewards.mean().item(), \"regret\": batch_regret.mean().item(), \"avg_regret\": regrets.mean()}\n",
    "    )\n",
    "\n",
    "    # 5. Update the bandit\n",
    "    bandit_module.record_feedback(chosen_contextualized_actions, realized_rewards, chosen_actions)\n",
    "    trainer.fit(bandit_module)\n",
    "    bandit_module = bandit_module.to(accelerator)\n",
    "\n",
    "# Store metrics\n",
    "metrics = pd.DataFrame(\n",
    "    {\n",
    "        \"reward\": rewards,\n",
    "        \"regret\": regrets,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate cumulative metrics\n",
    "cumulative_reward = np.cumsum(metrics[\"reward\"])\n",
    "cumulative_regret = np.cumsum(metrics[\"regret\"])\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cumulative_reward[:1000], label=\"reward\")\n",
    "plt.plot(cumulative_regret[:1000], label=\"regret\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"cumulative reward/regret\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print average metrics at different time horizons\n",
    "print(f\"Average reward (first 10 rounds): {np.mean(metrics['reward'][:10]):.4f}\")\n",
    "print(f\"Average reward (first 100 rounds): {np.mean(metrics['reward'][:100]):.4f}\")\n",
    "print(f\"Average reward (all rounds): {np.mean(metrics['reward']):.4f}\")\n",
    "print(\"\")\n",
    "print(f\"Average regret (first 10 rounds): {np.mean(metrics['regret'][:10]):.4f}\")\n",
    "print(f\"Average regret (first 100 rounds): {np.mean(metrics['regret'][:100]):.4f}\")\n",
    "print(f\"Average regret (all rounds): {np.mean(metrics['regret']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_bandits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
