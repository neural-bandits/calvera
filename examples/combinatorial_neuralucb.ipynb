{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import lightning as pl\n",
    "from lightning.pytorch.loggers.csv_logs import CSVLogger\n",
    "\n",
    "from calvera.benchmark.datasets.movie_lens import MovieLensDataset\n",
    "from calvera.bandits.neural_ucb_bandit import NeuralUCBBandit\n",
    "from calvera.utils.selectors import TopKSelector\n",
    "from calvera.benchmark.environment import BanditBenchmarkEnvironment\n",
    "from calvera.benchmark.logger_decorator import OnlineBanditLoggerDecorator\n",
    "from calvera.utils.data_storage import InMemoryDataBuffer, SlidingWindowBufferStrategy\n",
    "from calvera.benchmark.datasets.synthetic_combinatorial import SyntheticCombinatorialDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, dim, hidden_size=100):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_size)\n",
    "        self.activate = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.activate(self.fc1(x)))\n",
    "\n",
    "\n",
    "class LinearNetwork(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SyntheticCombinatorialDataset(\n",
    "    n_samples=4000, num_actions=20, context_size=40, function_type=\"quadratic\", noise_std=0.1, seed=43\n",
    ")\n",
    "\n",
    "contexts, rewards = dataset[0]\n",
    "print(f\"\\nContext shape: {contexts.shape}\")\n",
    "print(f\"Reward shape: {rewards.shape}\")\n",
    "\n",
    "print(f\"\\nRewards: {rewards[:].tolist()}\")\n",
    "\n",
    "K = 4\n",
    "top_k_indices = torch.topk(rewards, K).indices\n",
    "print(f\"\\nTop {K} arms: {top_k_indices.tolist()}\")\n",
    "print(f\"Top {K} rewards: {rewards[top_k_indices].tolist()}\")\n",
    "\n",
    "# Plot first two dimensions of X and color by y\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    dataset.contexts.view(-1, dataset.context_size)[:, 0],\n",
    "    dataset.contexts.view(-1, dataset.context_size)[:, 1],\n",
    "    c=dataset.rewards.view(-1, dataset.context_size)[:],\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "plt.xlabel(\"Context dimension 1\")\n",
    "plt.ylabel(\"Context dimension 2\")\n",
    "plt.title(\"Context vectors colored by rewards\")\n",
    "plt.colorbar(label=\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MovieLensDataset(version=\"ml-32m\")\n",
    "\n",
    "K = 4\n",
    "\n",
    "train_loader = DataLoader(Subset(dataset, range(10000)), batch_size=1, shuffle=True)\n",
    "env = BanditBenchmarkEnvironment(train_loader)\n",
    "\n",
    "# buffer = InMemoryDataBuffer(\n",
    "#     buffer_strategy=AllDataBufferStrategy(),\n",
    "#     max_size=None,\n",
    "# )\n",
    "\n",
    "buffer = InMemoryDataBuffer(\n",
    "    buffer_strategy=SlidingWindowBufferStrategy(window_size=1000),\n",
    ")\n",
    "\n",
    "network = Network(dataset.context_size, hidden_size=100)\n",
    "# network = LinearNetwork(dataset.context_size)\n",
    "\n",
    "bandit_module = NeuralUCBBandit(\n",
    "    n_features=dataset.context_size,\n",
    "    network=network,\n",
    "    selector=TopKSelector(k=K),\n",
    "    buffer=buffer,\n",
    "    train_batch_size=64,\n",
    "    early_stop_threshold=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    exploration_rate=3,\n",
    "    learning_rate=1e-3,\n",
    "    min_samples_required_for_training=1,\n",
    "    initial_train_steps=0,\n",
    ")\n",
    "\n",
    "logging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.FATAL)\n",
    "logging.getLogger(\"neural_bandits.bandits.abstract_bandit\").setLevel(logging.FATAL)\n",
    "logging.getLogger(\"neural_bandits.bandits.neural_bandit\").setLevel(logging.FATAL)\n",
    "logger = OnlineBanditLoggerDecorator(\n",
    "    CSVLogger(\"logs\", name=\"combinatorial_neural_ucb_bandit\", flush_logs_every_n_steps=100),\n",
    "    enable_console_logging=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([])\n",
    "regrets = np.array([])\n",
    "progress_bar = tqdm(enumerate(env), total=len(env))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "scatter = ax.scatter([], [], c=[], cmap=\"viridis\")\n",
    "cbar = fig.colorbar(scatter, ax=ax, label=\"True Reward (o)\")\n",
    "cbar2 = fig.colorbar(scatter, ax=ax, label=\"Pred Reward (x)\")\n",
    "\n",
    "for i, contextualized_actions in progress_bar:\n",
    "    chosen_actions, _ = bandit_module.forward(contextualized_actions)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=1,\n",
    "        max_steps=1024,\n",
    "        logger=logger,\n",
    "        gradient_clip_val=20.0,\n",
    "        log_every_n_steps=1,\n",
    "        enable_progress_bar=False,\n",
    "        enable_model_summary=False,\n",
    "        enable_checkpointing=False,\n",
    "    )\n",
    "\n",
    "    chosen_contextualized_actions, realized_scores = env.get_feedback(chosen_actions)\n",
    "    realized_rewards = realized_scores.sum(dim=1)\n",
    "    batch_regret = env.compute_regret(chosen_actions)\n",
    "\n",
    "    rewards = np.append(rewards, realized_rewards.cpu().numpy())\n",
    "    regrets = np.append(regrets, batch_regret.cpu().numpy())\n",
    "\n",
    "    progress_bar.set_postfix(\n",
    "        reward=realized_rewards.mean().item(),\n",
    "        regret=batch_regret.mean().item(),\n",
    "        avg_regret=np.mean(regrets),\n",
    "        acc_regret=np.sum(regrets),\n",
    "    )\n",
    "\n",
    "    bandit_module.record_feedback(chosen_contextualized_actions, realized_scores)\n",
    "    trainer.fit(bandit_module)\n",
    "\n",
    "    pred_y = bandit_module.theta_t(buffer.contextualized_actions).detach().numpy()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        ax.clear()\n",
    "\n",
    "        scatter_true = ax.scatter(\n",
    "            buffer.contextualized_actions[:, 0, 0],\n",
    "            buffer.contextualized_actions[:, 0, 1],\n",
    "            c=buffer.rewards,\n",
    "            cmap=\"viridis\",\n",
    "            marker=\"o\",\n",
    "            label=\"True Rewards\",\n",
    "            alpha=1.0,\n",
    "        )\n",
    "\n",
    "        # Plot first two dimensions of X and color by y\n",
    "        scatter_pred = ax.scatter(\n",
    "            buffer.contextualized_actions[:, 0, 0],\n",
    "            bandit_module.buffer.contextualized_actions[:, 0, 1],\n",
    "            c=pred_y,\n",
    "            marker=\"x\",\n",
    "            cmap=\"viridis\",\n",
    "            alpha=0.4,\n",
    "        )  # type: ignore\n",
    "        ax.set_xlabel(\"Context dimension 1\")\n",
    "        ax.set_ylabel(\"Context dimension 2\")\n",
    "        ax.set_title(f\"Context vectors colored by rewards (iteration {i})\")\n",
    "\n",
    "        cbar.update_normal(scatter_true)\n",
    "        cbar2.update_normal(scatter_pred)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    {\n",
    "        \"reward\": rewards,\n",
    "        \"regret\": regrets,\n",
    "    }\n",
    ")\n",
    "\n",
    "metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative metrics\n",
    "cumulative_reward = np.cumsum(metrics[\"reward\"])\n",
    "cumulative_regret = np.cumsum(metrics[\"regret\"])\n",
    "\n",
    "# Plot cumulative reward and regret\n",
    "plt.plot(cumulative_reward, label=\"Cumulative Reward\")\n",
    "plt.plot(cumulative_regret, label=\"Cumulative Regret\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"cumulative reward/regret\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average reward (first 10 rounds): {np.mean(metrics['reward'][:10]):.4f}\")\n",
    "print(f\"Average reward (first 100 rounds): {np.mean(metrics['reward'][:100]):.4f}\")\n",
    "print(f\"Average reward (all rounds): {np.mean(metrics['reward']):.4f}\")\n",
    "print(\"\")\n",
    "print(f\"Average regret (first 10 rounds): {np.mean(metrics['regret'][:10]):.4f}\")\n",
    "print(f\"Average regret (first 100 rounds): {np.mean(metrics['regret'][:100]):.4f}\")\n",
    "print(f\"Average regret (all rounds): {np.mean(metrics['regret']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_metrics_csv = logger._logger_wrappee.log_dir + \"/metrics.csv\"\n",
    "print(bandit_metrics_csv)\n",
    "bandit_metrics = pd.read_csv(bandit_metrics_csv)\n",
    "\n",
    "plt.plot(bandit_metrics[\"loss\"][:10000].dropna(), label=\"Loss\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_bandits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
