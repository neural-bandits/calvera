{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import lightning as pl\n",
                "from torch.utils.data import DataLoader, Subset\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "\n",
                "from neural_bandits.bandits.linear_ucb_bandit import LinearUCBBandit\n",
                "from neural_bandits.benchmark.datasets.statlog import StatlogDataset\n",
                "\n",
                "from neural_bandits.benchmark.environment import BanditBenchmarkEnvironment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = StatlogDataset()\n",
                "print(dataset.context_size)\n",
                "print(len(dataset))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from neural_bandits.utils.selectors import EpsilonGreedySelector\n",
                "\n",
                "train_loader = DataLoader(Subset(dataset, range(5000)), batch_size=32, shuffle=True)\n",
                "\n",
                "accelerator = \"cpu\"\n",
                "env = BanditBenchmarkEnvironment(train_loader, device=accelerator)\n",
                "\n",
                "bandit_module = LinearUCBBandit(\n",
                "    n_features=dataset.context_size,\n",
                "    selector=EpsilonGreedySelector(0.1),\n",
                ").to(accelerator)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "metrics = []\n",
                "progress = tqdm(env, total=len(env))\n",
                "for contextualized_actions in progress:\n",
                "    chosen_actions, _ = bandit_module.forward(contextualized_actions)\n",
                "\n",
                "    trainer = pl.Trainer(\n",
                "        max_epochs=1,\n",
                "        enable_progress_bar=False,\n",
                "        enable_model_summary=False,\n",
                "        accelerator=accelerator,\n",
                "    )\n",
                "    chosen_contextualized_actions, realized_rewards = env.get_feedback(chosen_actions)\n",
                "    batch_regret = env.compute_regret(chosen_actions)\n",
                "\n",
                "    metrics.append({\"reward\": realized_rewards.sum().item(), \"regret\": batch_regret.sum().item()})\n",
                "    progress.set_postfix({\"reward\": realized_rewards.mean().item(), \"regret\": realized_rewards.mean().item()})\n",
                "\n",
                "    bandit_module.record_feedback(chosen_contextualized_actions, realized_rewards)\n",
                "    trainer.fit(bandit_module)\n",
                "    # Because of this: https://github.com/Lightning-AI/pytorch-lightning/issues/10294,\n",
                "    # we need to move the model to the desired device.\n",
                "    bandit_module = bandit_module.to(accelerator)\n",
                "metrics = pd.DataFrame(metrics)\n",
                "metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load metrics from the logger and plot\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "cumulative_reward = np.cumsum(metrics[\"reward\"][:5000])\n",
                "cumulative_regret = np.cumsum(metrics[\"regret\"][:5000].dropna())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.plot(cumulative_reward, label=\"reward\")\n",
                "plt.plot(cumulative_regret, label=\"regret\")\n",
                "plt.xlabel(\"steps\")\n",
                "plt.ylabel(\"cumulative reward/regret\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# average reward\n",
                "print(sum(metrics[\"reward\"][:100]) / 100)\n",
                "print(sum(metrics[\"reward\"][:1000]) / 1000)\n",
                "print(sum(metrics[\"reward\"][:10000]) / 10000)\n",
                "print(sum(metrics[\"regret\"][:100].dropna()) / 100)\n",
                "print(sum(metrics[\"regret\"][:1000].dropna()) / 1000)\n",
                "print(sum(metrics[\"regret\"][:10000].dropna()) / 10000)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "neural_bandits",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
