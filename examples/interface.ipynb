{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mypy: ignore-errors\n",
    "# from neural_bandits.modules.linear_bandit_module import LinearBanditModule\n",
    "# from neural_bandits.algorithms.linear_bandits import LinearUCBBandit\n",
    "# from neural_bandits.benchmark.datasets.statlog import StatlogDataset\n",
    "\n",
    "# import lightning as pl\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpi/fs00/home/robert.weeke/miniconda3/envs/neural_bandits/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /hpi/fs00/home/robert.weeke/miniconda3/envs/neural_b ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "58000\n"
     ]
    }
   ],
   "source": [
    "# # mypy: ignore-errors\n",
    "# dataset = StatlogDataset()\n",
    "# print(dataset.context_size)\n",
    "# print(len(dataset))\n",
    "\n",
    "# train_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "# model = LinearBanditModule(\n",
    "#         linear_bandit_type = LinearUCBBandit,\n",
    "#         n_features = dataset.context_size * dataset.num_actions,\n",
    "# )\n",
    "# logger = pl.pytorch.loggers.CSVLogger(\"logs\", name=\"linear_bandit\", flush_logs_every_n_steps=100)\n",
    "# trainer = pl.Trainer(max_epochs=1, logger=logger, log_every_n_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mypy: ignore-errors\n",
    "\n",
    "# # TODO: Create a bunch of selectors in the utils\n",
    "# Selector = lambda x: x.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adjust the training_step method of each module to take:\n",
    "#  - tuple of (chosen_contextualized_actions [batch_size, num_chosen_actions, num_features], realized_reward [batch_size, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mypy: ignore-errors\n",
    "\n",
    "# # USER CODE (\"Online\" Learning with Delayed Feedback from Logged Data of Linear Models)\n",
    "# # dataset = Dataset.LoadFromDB()  # Bandit Feedback: this contains the last +- 1000 samples but we don't know the exact size\n",
    "# train_loader = DataLoader(dataset, batch_size=32, shuffle=True) # this loads a bunch of mini-batches. we dont know how big the dataset is.\n",
    "# trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mypy: ignore-errors\n",
    "\n",
    "# # TODO: make nice classes\n",
    "# class LinearBenchmark:\n",
    "#     def __init__(self, model, trainer, train_loader):\n",
    "#         self.model = model\n",
    "#         self.trainer = trainer\n",
    "#         self.train_loader = train_loader # MNIST of 100000 samples, split into mini-batches of 32 (or 1)\n",
    "\n",
    "#     def run(self):\n",
    "#         for batch_idx, (contextualized_actions, rewards) in enumerate(self.train_loader):\n",
    "#             action_distribution = self.model(contextualized_actions)\n",
    "#             actions = Selector(action_distribution)\n",
    "#             realized_rewards = rewards[torch.arange(len(rewards)), actions]\n",
    "#             chosen_actions = contextualized_actions[torch.arange(len(contextualized_actions)), actions]\n",
    "#             self.on_batch(chosen_actions, realized_rewards)\n",
    "\n",
    "#     def on_batch(\n",
    "#         self,\n",
    "#         chosen_actions: torch.Tensor,  # shape (batch_size, num_chosen_actions)\n",
    "#         realized_rewards: torch.Tensor,  # shape (batch_size,)\n",
    "#     ):\n",
    "#         batch_dataset = torch.utils.data.TensorDataset(chosen_actions, realized_rewards)\n",
    "#         batch_data_loader = torch.utils.data.DataLoader(\n",
    "#             batch_dataset,\n",
    "#             batch_size=len(batch_dataset),\n",
    "#         )\n",
    "#         self.trainer.fit(self.model, batch_data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mypy: ignore-errors\n",
    "\n",
    "# # USER CODE (\"Online\" Learning with Delayed Feedback from Logged Data)\n",
    "# # dataset = Dataset.LoadFromDB()  # Bandit Feedback: this contains the last +- 1000 samples but we don't know the exact size\n",
    "# train_loader = DataLoader(dataset, batch_size=32, shuffle=True) # this loads a bunch of mini-batches. we dont know how big the dataset is.\n",
    "# trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mypy: ignore-errors\n",
    "# class NeuralUCBBenchmark:\n",
    "#     def __init__(self, model, trainer, train_loader):\n",
    "#         self.model = model\n",
    "#         self.trainer = trainer\n",
    "#         self.train_loader = train_loader\n",
    "#         self.mini_batch_size = 32\n",
    "\n",
    "#     def run(self):\n",
    "#         for batch_idx, (contextualized_actions, rewards) in enumerate(self.train_loader):\n",
    "#             action_distribution = self.model(contextualized_actions)\n",
    "#             actions = Selector(action_distribution)\n",
    "#             realized_rewards = rewards[torch.arange(len(rewards)), actions]\n",
    "#             chosen_actions = contextualized_actions[torch.arange(len(contextualized_actions)), actions]\n",
    "#             self.on_batch(chosen_actions, realized_rewards)\n",
    "            \n",
    "#             if batch_idx % 100 == 0:\n",
    "#                 self.on_interval()\n",
    "\n",
    "#     def on_batch(\n",
    "#         self,\n",
    "#         chosen_actions: torch.Tensor,  # shape (batch_size, num_chosen_actions)\n",
    "#         realized_rewards: torch.Tensor,  # shape (batch_size,)\n",
    "#     ):\n",
    "#         # option 2\n",
    "#         self.add_data(chosen_actions, realized_rewards)\n",
    "\n",
    "#     def on_interval(\n",
    "#         self,\n",
    "#     ):\n",
    "#         # option 2\n",
    "#         data_loader = DataLoader(Dataset(self.data))\n",
    "#         self.trainer.fit(self.model, data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # USER CODE: NeuralUCB from delayed feedback. OPTION 1: Bandit decides itself when to update\n",
    "# trainer = pl.Trainer(max_epochs=1, logger=logger, log_every_n_steps=1)\n",
    "\n",
    "# model = NeuralUCB(\n",
    "#     initial_training_steps=1000,  # number of data points\n",
    "#     update_freq=1000, # number of data points\n",
    "#     train_data_strategy=AllDataStrategy() | SlidingWindowDataStrategy(data_points=100), # last = this data\n",
    "#     warm_start=True,nn=torch.nn.Sequential([])\n",
    "# )\n",
    "\n",
    "# # dataloader: each batch: 32 * (chosen_action, realized_reward)\n",
    "# # mini-batch 1: {(x, y), (x, y), ... 32 times} -> maybe update Neural Network, store batch in storage\n",
    "# # mini-batch 2: {(x, y), (x, y), ... 32 times} -> maybe update Neural Network (can use last batch as well by loading from storage)\n",
    "# # ...\n",
    "\n",
    "# # for existing dataset\n",
    "# trainer.fit(model, dataloader) # sometimes it doesn't do anything, once there is enough new data it will update the model\n",
    "\n",
    "# # training + inference at once loop\n",
    "# logger = Logger()\n",
    "# for i in range(1000):\n",
    "#     actions = model(data)\n",
    "#     rewards = environment.get_rewards(actions)\n",
    "#     trainer = Trainer(logger) # unfortunately, we need to create a new trainer every time because it can't handle being called with a different data loader\n",
    "#     trainer.fit(model, DataLoader(Dataset((actions, rewards))))\n",
    "#     # -> can't use batch_idx\n",
    "#     # -> need a warning if the model hasn't updated\n",
    "    \n",
    "# # INFERENCE LOOP\n",
    "# actions = model(context)\n",
    "# # return to user\n",
    "\n",
    "# # STREAM OF USER REWARDS\n",
    "# database.add_reward(action, realized_reward)\n",
    "\n",
    "# # TRAINER\n",
    "# trainer.fit(model, DataLoader(Dataset(database.get_rewards())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other possible interfaces we do not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################### OLD ##########################################################\n",
    "# # USER CODE: NeuralUCB from delayed feedback. OPTION 2: Bandit stores data and user decides when to update\n",
    "# trainer = pl.Trainer(max_epochs=1, logger=logger, log_every_n_steps=1)\n",
    "\n",
    "# model = NeuralUCB(\n",
    "#     update_freq=None,\n",
    "#     train_data_strategy=DataStrategy.ALL | DataStrategy.SLIDING_WINDOW | DataStrategy.LAST_DATA,\n",
    "# )\n",
    "\n",
    "# # dataset: n * (chosen_action, realized_reward)\n",
    "# # throughout the day\n",
    "# model.add_data(dataset)\n",
    "\n",
    "# # on midnight or when enough data is collected\n",
    "# trainer.fit(model) # loads the data from self.train_dataloader() automatically (depending on the data strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USER CODE: NeuralUCB from delayed feedback. OPTION 3: User stores data and user decides when to update\n",
    "# trainer = pl.Trainer(max_epochs=1, logger=logger, log_every_n_steps=1)\n",
    "\n",
    "# model = NeuralUCB(\n",
    "#     update_freq=None,\n",
    "#     train_data_strategy=DataStrategy.LAST_DATA,\n",
    "# )\n",
    "\n",
    "# # data_loader: n batches of size m of (chosen_action, realized_reward)\n",
    "# trainer.fit(model, your_dataloader) # loads the data from self.train_dataloader() automatically (depending on the data strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another function for retrieving from storage\n",
    "# your_data = model.get_data() or model.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USER CODE: NeuralLinear from delayed feedback\n",
    "# trainer = pl.Trainer(max_epochs=1, logger=logger, log_every_n_steps=1)\n",
    "\n",
    "# model = NeuralLinear(\n",
    "#     update_encoder_freq=1000,\n",
    "#     update_head_freq=64,\n",
    "#     train_data_strategy=DataStrategy.ALL | DataStrategy.SLIDING_WINDOW | DataStrategy.MINI_BATCH,\n",
    "# )\n",
    "\n",
    "# # train_loader: each batch: 32 * (chosen_action, realized_reward)\n",
    "# # mini-batch 1: {(x, y), (x, y), ... 32 times} -> update head, store batch in storage\n",
    "# # mini-batch 2: {(x, y), (x, y), ... 32 times} -> update head (can use last batch as well by loading from storage?)\n",
    "# # ...\n",
    "\n",
    "# # model.set_update_freq(100)\n",
    "# model.retrain()\n",
    "# trainer.fit(model, train_loader)\n",
    "\n",
    "# storage_loader = DataLoader(storage)\n",
    "# model.force_retrain() # from storage\n",
    "# trainer.fit(model, storage_loader)\n",
    "\n",
    "\n",
    "# # for batch_idx, (contextualized_actions, rewards) in enumerate(train_loader):\n",
    "\n",
    "# #     if batch_idx % 100 != 0:\n",
    "# #         batch_dataset = torch.utils.data.TensorDataset(contextualized_actions, rewards)\n",
    "# #         trainer.fit(model, batch_dataset)\n",
    "# #     else:\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeuralLinearBenchmark:\n",
    "#     def __init__(self, model, trainer, train_loader):\n",
    "#         self.model = model\n",
    "#         self.trainer = trainer\n",
    "#         self.train_loader = train_loader\n",
    "#         self.mini_batch_size = 32\n",
    "\n",
    "#     def run(self):\n",
    "#         for batch_idx, (contextualized_actions, rewards) in enumerate(self.train_loader):\n",
    "#             action_distribution = self.model(contextualized_actions)\n",
    "#             actions = Selector(action_distribution)\n",
    "#             realized_rewards = rewards[torch.arange(len(rewards)), actions]\n",
    "#             chosen_actions = contextualized_actions[torch.arange(len(contextualized_actions)), actions]\n",
    "#             self.on_batch(chosen_actions, realized_rewards)\n",
    "            \n",
    "#             if batch_idx % 100 == 0:\n",
    "#                 self.on_interval()\n",
    "\n",
    "#     def on_batch(\n",
    "#         self,\n",
    "#         chosen_actions: torch.Tensor,  # shape (batch_size, num_chosen_actions)\n",
    "#         realized_rewards: torch.Tensor,  # shape (batch_size,)\n",
    "#     ):\n",
    "#         batch_dataset = torch.utils.data.TensorDataset(chosen_actions, realized_rewards)\n",
    "#         batch_data_loader = torch.utils.data.DataLoader(\n",
    "#             batch_dataset,\n",
    "#             batch_size=len(batch_dataset),\n",
    "#         )\n",
    "#         self.trainer.fit(self.model, batch_data_loader)\n",
    "\n",
    "#     def on_interval(\n",
    "#         self,\n",
    "#     ):\n",
    "#         self.model.train_encoder()\n",
    "#         self.trainer.fit(self.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### PROPOSAL (rob2u):\n",
    "# # IDK if this will work, although I do not see a reason why it shouldnt.\n",
    "# # The issue is that we require the user to perform the update of the head and the encoder manually.\n",
    "\n",
    "# class BanditTrain(pl.LightningModule):\n",
    "#     def __init__(self, model):\n",
    "#         self.neural_net = model.neural_net\n",
    "#         self.head = model.head\n",
    "#         self.mini_batch_size = 32\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "    \n",
    "#     def on_train_epoch_start(self):\n",
    "#         self.head.reset()\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         chosen_action_contexts, realized_rewards = batch\n",
    "        \n",
    "#         # send through feature extractor (neural network)\n",
    "#         x = self.feature_extractor(chosen_action_contexts)\n",
    "        \n",
    "#         # send through head\n",
    "#         self.head.update(x, realized_rewards)\n",
    "        \n",
    "#         return torch.tensor(0.0)\n",
    "\n",
    "\n",
    "# class NeuralLinearBenchmark:\n",
    "#     def __init__(self, model, trainer_linear_bandit, trainer_neural_net, train_loader):\n",
    "#         self.model = model\n",
    "#         self.trainer_neural_net = trainer_neural_net\n",
    "        \n",
    "#         self.bandit_train_module = BanditTrain(model)\n",
    "#         self.trainer_linear_bandit = trainer_linear_bandit\n",
    "\n",
    "#         self.train_loader = train_loader\n",
    "#         self.mini_batch_size = 32\n",
    "\n",
    "#     def run(self):\n",
    "#         for batch_idx, (contextualized_actions, rewards) in enumerate(self.train_loader):\n",
    "#             action_distribution = self.model(contextualized_actions)\n",
    "#             actions = Selector(action_distribution)\n",
    "#             realized_rewards = rewards[torch.arange(len(rewards)), actions]\n",
    "#             chosen_actions = contextualized_actions[torch.arange(len(contextualized_actions)), actions]\n",
    "#             self.on_batch(chosen_actions, realized_rewards)\n",
    "            \n",
    "#             if batch_idx % 100 == 0:\n",
    "#                 self.on_interval()\n",
    "\n",
    "#     def on_batch(\n",
    "#         self,\n",
    "#         chosen_actions: torch.Tensor,  # shape (batch_size, num_chosen_actions)\n",
    "#         realized_rewards: torch.Tensor,  # shape (batch_size,)\n",
    "#     ):\n",
    "#         batch_dataset = torch.utils.data.TensorDataset(chosen_actions, realized_rewards)\n",
    "#         batch_data_loader = torch.utils.data.DataLoader(\n",
    "#             batch_dataset,\n",
    "#             batch_size=len(batch_dataset),\n",
    "#         )\n",
    "#         self.trainer_linear_bandit.fit(self.bandit_train_module, self.model.train_dataloader())\n",
    "\n",
    "#     def on_interval(\n",
    "#         self,\n",
    "#     ):\n",
    "#         self.trainer.fit(self.model)\n",
    "#         self.trainer_linear_bandit.fit(self.bandit_train_module, self.model.train_dataloader())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### PROPOSAL (philipp)\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class BanditNetwork(pl.LightningModule):\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         chosen_action_contexts, realized_rewards = batch\n",
    "        \n",
    "#         z = self.forward(chosen_action_contexts)\n",
    "#         loss = nn.functional.mse_loss(z, realized_rewards)\n",
    "        \n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# class NeuralLinearBenchmark:\n",
    "#     def __init__(self, encoder_model, trainer_linear_bandit, trainer_neural_net, train_loader):\n",
    "#         self.model = NeuralLinear(BanditNetwork(encoder_model))\n",
    "#         self.trainer_neural_net = trainer_neural_net\n",
    "#         self.trainer_linear_bandit = trainer_linear_bandit\n",
    "\n",
    "#         self.train_loader = train_loader\n",
    "#         self.mini_batch_size = 32\n",
    "\n",
    "#     def run(self):\n",
    "#         for batch_idx, (contextualized_actions, rewards) in enumerate(self.train_loader):\n",
    "#             action_distribution = self.model(contextualized_actions)\n",
    "#             actions = Selector(action_distribution)\n",
    "#             realized_rewards = rewards[torch.arange(len(rewards)), actions]\n",
    "#             chosen_actions = contextualized_actions[torch.arange(len(contextualized_actions)), actions]\n",
    "#             self.on_batch(chosen_actions, realized_rewards)\n",
    "            \n",
    "#             if batch_idx % 100 == 0:\n",
    "#                 self.on_interval()\n",
    "\n",
    "#     def on_batch(\n",
    "#         self,\n",
    "#         chosen_actions: torch.Tensor,  # shape (batch_size, num_chosen_actions)\n",
    "#         realized_rewards: torch.Tensor,  # shape (batch_size,)\n",
    "#     ):\n",
    "#         batch_dataset = torch.utils.data.TensorDataset(chosen_actions, realized_rewards)\n",
    "#         batch_data_loader = torch.utils.data.DataLoader(\n",
    "#             batch_dataset,\n",
    "#             batch_size=len(batch_dataset),\n",
    "#         )\n",
    "#         # using option 2a\n",
    "#         self.trainer_linear_bandit.fit(self.model, batch_data_loader)\n",
    "\n",
    "#     def on_interval(\n",
    "#         self,\n",
    "#     ):\n",
    "#         self.trainer_neural_net.fit(self.model.encoder, self.model.train_dataloader())\n",
    "#         self.model.update_encoder_data() # recompute all encodings. TODO: Can we compute them \"lazy\"? e.g. by invalidating them: self.model.invalidate_encodings()\n",
    "#         self.trainer_linear_bandit.fit(self.model, self.model.encoder.train_dataloader()) # IMPORTANT to use the data from the encoder because only it contains the pairs of (chosen_action, realized_reward)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### USER CODE (philipp): OPTION 1: Bandit decides itself when to update\n",
    "# trainer = pl.Trainer(max_epochs=1, logger=logger, log_every_n_steps=1)\n",
    "\n",
    "# model = NeuralLinear(\n",
    "#     encoder = BanditNetwork(encoder_model),\n",
    "#     update_encoder_freq=1000,\n",
    "#     update_head_freq=64,\n",
    "#     train_data_strategy=DataStrategy.ALL | DataStrategy.SLIDING_WINDOW | DataStrategy.MINI_BATCH,\n",
    "# )\n",
    "\n",
    "# # train_loader: each batch: 32 * (chosen_action, realized_reward)\n",
    "# # mini-batch 1: {(x, y), (x, y), ... 32 times} -> update head, store batch in storage\n",
    "# # mini-batch 2: {(x, y), (x, y), ... 32 times} -> update head (can use last batch as well by loading from storage?)\n",
    "# # ...\n",
    "\n",
    "# # Optional overwrite of the update frequencies\n",
    "# # model.set_update_encoder_freq(100)\n",
    "# # model.set_update_head_freq(32)\n",
    "\n",
    "# trainer.fit(model, train_loader)\n",
    "# # 1. adds the data to the storage\n",
    "# # 2. if train_encoder_freq is reached, trains the encoder on pairs of context_actions and rewards. Then invalidates the encodings/retrains the head on pairse of updated embeddings and rewards\n",
    "# # 3. elif train_head_freq is reached, trains the head on pairs of embeddings and rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USER CODE: NeuralLinear from delayed feedback. OPTION 2: Bandit stores data and user decides when to update\n",
    "# trainer = pl.Trainer(max_epochs=1, logger=logger, log_every_n_steps=1)\n",
    "# encoder_trainer = pl.Trainer(max_epochs=1, logger=logger, log_every_n_steps=1)\n",
    "\n",
    "# model = NeuralLinear(\n",
    "#     encoder = BanditNetwork(encoder_model),\n",
    "#     update_encoder_freq=None,\n",
    "#     update_head_freq=None,\n",
    "#     train_data_strategy=DataStrategy.ALL | DataStrategy.SLIDING_WINDOW | DataStrategy.LAST_DATA,\n",
    "# )\n",
    "\n",
    "# # dataset: n * (chosen_action, realized_reward)\n",
    "# # throughout the day\n",
    "# model.add_data(dataset)\n",
    "\n",
    "# # on midnight or when enough data is collected\n",
    "# trainer.fit(model) # loads the data from self.train_dataloader() automatically (depending on the data strategy)\n",
    "# # 1. data already in storage, so no need to add it again (but we need some sort of ID!)\n",
    "# # 2. trains the head on pairs of embeddings and rewards. if any data is invalidated -> also call the encoder and save result\n",
    "\n",
    "# # rarely update the encoder\n",
    "# encoder_trainer.fit(model.encoder, model.encoder.train_dataloader())\n",
    "# # 1. data already in storage, so no need to add it again\n",
    "# # 2. trains the encoder on pairs of context_actions and rewards.\n",
    "# # 3. Then invalidate the embeddings. TODO QUESTION? Can we do this automatically after training the encoder? Would need a reference to the NeuralLinear model...\n",
    "# # Otherwise: user needs to call: model.invalidate_encodings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USER CODE: NeuralLinear from delayed feedback. OPTION 2a: Bandit stores data and user decides when to update but only for network\n",
    "# trainer = pl.Trainer(max_epochs=1, logger=logger, log_every_n_steps=1)\n",
    "# encoder_trainer = pl.Trainer(max_epochs=1, logger=logger, log_every_n_steps=1)\n",
    "\n",
    "# model = NeuralLinear(\n",
    "#     encoder = BanditNetwork(encoder_model),\n",
    "#     update_encoder_freq=None,\n",
    "#     update_head_freq=16,\n",
    "#     train_data_strategy=DataStrategy.ALL | DataStrategy.SLIDING_WINDOW | DataStrategy.LAST_DATA,\n",
    "# )\n",
    "\n",
    "# # train_loader: each batch: 32 * (chosen_action, realized_reward)\n",
    "# # mini-batch 1: {(x, y), (x, y), ... 32 times} -> update head, store batch in storage\n",
    "# # mini-batch 2: {(x, y), (x, y), ... 32 times} -> update head (can use last batch as well by loading from storage?)\n",
    "# # ...\n",
    "# trainer.fit(model, train_loader)\n",
    "# # 1. adds the data to the storage\n",
    "# # 2. if train_head_freq is reached, trains the head on pairs of embeddings and rewards. if any data is invalidated -> also call the encoder and save result\n",
    "\n",
    "# # on midnight or when enough data is collected\n",
    "# encoder_trainer.fit(model.encoder)\n",
    "# # 1. data already in storage, so no need to add it again\n",
    "# # 2. trains the encoder on pairs of context_actions and rewards.\n",
    "# # 3. Then invalidate the embeddings. TODO QUESTION? Can we do this automatically after training the encoder? Would need a reference to the NeuralLinear model...\n",
    "# # Otherwise: user needs to call: model.invalidate_encodings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER CODE: NeuralLinear from delayed feedback. OPTION 2b: Bandit stores data and user decides when to update but only for head. Seems to be rare...\n",
    "\n",
    "# NOT SUPPORTED!\n",
    "# That would require us to update the encoder in \"add_data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USER CODE: NeuralUCB from delayed feedback. OPTION 3: User stores data and user decides when to update\n",
    "# trainer = pl.Trainer(max_epochs=1, logger=logger, log_every_n_steps=1)\n",
    "# encoder_trainer = pl.Trainer(max_epochs=1, logger=logger, log_every_n_steps=1)\n",
    "\n",
    "# model = NeuralUCB(\n",
    "#     encoder = BanditNetwork(encoder_model),\n",
    "#     update_encoder_freq=None,\n",
    "#     update_head_freq=None,\n",
    "#     train_data_strategy=DataStrategy.LAST_DATA,\n",
    "# )\n",
    "\n",
    "# # Either update head:\n",
    "# # data_loader: n batches of size m of (chosen_action, realized_reward)\n",
    "# trainer.fit(model, your_dataloader)\n",
    "# # 1. TODO QUESTION: not sure if we should add the data anyways - even if we are in the setting where teh user stores the data. Can we differentiate between other cases? Don't think so...\n",
    "# # 2. trains the head on pairs of embeddings and rewards\n",
    "\n",
    "\n",
    "# # Or update encoder. Afterwards we need to update the head as well:\n",
    "# encoder_trainer.fit(model, your_dataloader)\n",
    "# # 1. trains the encoder on pairs of context_actions and rewards.\n",
    "# # no need to call invalidate_encodings() because the bandit does not store the data itself\n",
    "# your_data_loader.update_encodings(encoder_trainer) # smth like this is necessary!\n",
    "# trainer.fit(model, your_dataloader)\n",
    "# # 1. trains the head on pairs of embeddings and rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF WE DETECT THAT NOT ENOUGH DATA WAS IN THE DATASET, WE THROW A WARNING!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_bandits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
