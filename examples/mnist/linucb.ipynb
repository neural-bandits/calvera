{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    mnist = datasets.fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "    X, y = mnist.data, mnist.target\n",
    "    y = y.astype(np.int32)  # Convert target to integers\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "def avg_pooling(image, pool_size):\n",
    "    pooled_height = image.shape[0] // pool_size[0]\n",
    "    pooled_width = image.shape[1] // pool_size[1]\n",
    "    pooled_image = np.zeros((pooled_height, pooled_width))\n",
    "\n",
    "    for i in range(pooled_height):\n",
    "        for j in range(pooled_width):\n",
    "            start_i, end_i = i * pool_size[0], (i + 1) * pool_size[0]\n",
    "            start_j, end_j = j * pool_size[1], (j + 1) * pool_size[1]\n",
    "            pooled_image[i, j] = np.mean(image[start_i:end_i, start_j:end_j])\n",
    "\n",
    "    return pooled_image\n",
    "\n",
    "\n",
    "def preprocess_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train):\n",
    "    model = LogisticRegression(\n",
    "        max_iter=1000, solver=\"lbfgs\", multi_class=\"multinomial\", random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(pred_func, X_test, y_test):\n",
    "    y_pred = pred_func(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "def predict(model, new_data):\n",
    "    predictions = model.predict(new_data)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:03<00:00, 17515.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# Load and split the data\n",
    "X, y = load_data()\n",
    "\n",
    "pool_size = 7\n",
    "X_pooled = np.zeros((X.shape[0], (28 // pool_size) * (28 // pool_size)))\n",
    "for i in tqdm.tqdm(range(X.shape[0])):\n",
    "    image = X[i].reshape(28, 28)\n",
    "    pooled_image = avg_pooling(image, (pool_size, pool_size))\n",
    "    X_pooled[i] = pooled_image.flatten()\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X_pooled, y)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_scaled, X_test_scaled = preprocess_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robert/miniconda3/envs/neural_bandits/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = train_model(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7457142857142857\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.68      0.74      1343\n",
      "           1       0.82      0.90      0.86      1600\n",
      "           2       0.82      0.75      0.79      1380\n",
      "           3       0.79      0.77      0.78      1433\n",
      "           4       0.64      0.69      0.66      1295\n",
      "           5       0.68      0.61      0.64      1273\n",
      "           6       0.84      0.87      0.86      1396\n",
      "           7       0.77      0.81      0.79      1503\n",
      "           8       0.64      0.69      0.66      1357\n",
      "           9       0.64      0.64      0.64      1420\n",
      "\n",
      "    accuracy                           0.75     14000\n",
      "   macro avg       0.74      0.74      0.74     14000\n",
      "weighted avg       0.75      0.75      0.75     14000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 916   10   23   23   19   77   23    4  237   11]\n",
      " [   0 1446    6   36   13   50    3   10   24   12]\n",
      " [  11   33 1038   66   60   11   81   12   54   14]\n",
      " [  22   57   56 1099   17   45    3   67   31   36]\n",
      " [   5   42   31    2  890   32   55   33   15  190]\n",
      " [  55    5   11   67  129  776   39   37  106   48]\n",
      " [  29   29   40    2   53    7 1214    0   21    1]\n",
      " [   1   39   11   24   43   21    1 1217    9  137]\n",
      " [  65   53   37   59   11  112   21    5  933   61]\n",
      " [  14   56    7   19  156   18    3  200   36  911]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(model.predict, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_bandits.algorithms.linear_bandits import LinearTSBandit, LinearUCBBandit\n",
    "\n",
    "\n",
    "n_features = X_train_scaled.shape[1]\n",
    "alpha = 0.01\n",
    "n_arms = 10  # Number of classes\n",
    "bandit = LinearTSBandit(n_arms, n_features * n_arms)\n",
    "# bandit = LinearUCBBandit(n_arms, n_features * n_arms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56000/56000 [01:04<00:00, 866.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from neural_bandits.utils.multiclass import MultiClassContextualiser\n",
    "from neural_bandits.trainers.linear_trainer import LinearTrainer\n",
    "\n",
    "mc_contextualiser = MultiClassContextualiser(n_arms)\n",
    "trainer = LinearTrainer()\n",
    "\n",
    "for t in tqdm.tqdm(range(X_train_scaled.shape[0])):\n",
    "    x_tensor = torch.tensor(X_train_scaled[t], dtype=torch.float32).reshape(1, -1)\n",
    "    contextualised_actions = mc_contextualiser.contextualise(x_tensor)\n",
    "    chosen_arm = bandit(contextualised_actions)\n",
    "    reward = 1 if y_train[t] == chosen_arm else 0\n",
    "    reward = torch.tensor([reward], dtype=torch.float32)\n",
    "\n",
    "    trainer.update(bandit, reward, contextualised_actions[0, chosen_arm].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56000/56000 [00:24<00:00, 2274.29it/s]\n"
     ]
    }
   ],
   "source": [
    "batch = 1000\n",
    "buffer_reward = []\n",
    "buffer_contextualised_actions = []\n",
    "for t in tqdm.tqdm(range(X_train_scaled.shape[0])):\n",
    "    # for t in tqdm.tqdm(range(1)):\n",
    "    contextualised_actions = mc_contextualiser.contextualise(\n",
    "        torch.tensor(X_train_scaled[t], dtype=torch.float32).reshape(1, -1)\n",
    "    )\n",
    "    chosen_arm = bandit(contextualised_actions)\n",
    "    reward = 1 if y_train[t] == chosen_arm else 0\n",
    "    reward = torch.tensor([reward], dtype=torch.float32)\n",
    "    buffer_reward.append(reward)\n",
    "    buffer_contextualised_actions.append(contextualised_actions[:, chosen_arm])\n",
    "\n",
    "\n",
    "if t % batch == 99:\n",
    "    bandit = trainer.update(\n",
    "        bandit,\n",
    "        torch.cat(buffer_reward, dim=0),\n",
    "        torch.cat(buffer_contextualised_actions, dim=0),\n",
    "    )\n",
    "    buffer_reward = []\n",
    "    buffer_contextualised_actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.5911e-03, -1.1154e-02, -4.8883e-03,  1.2245e-02,  5.7645e-03,\n",
      "        -4.4941e-02, -3.6960e-02, -6.4851e-04,  3.2460e-03, -4.5446e-02,\n",
      "        -4.9883e-02, -8.1671e-02,  5.9538e-02,  8.7288e-02, -1.8538e-01,\n",
      "        -7.6898e-02,  4.5177e-02, -8.4394e-02,  7.9709e-02, -1.9640e-01,\n",
      "         8.8489e-02,  6.7506e-02,  3.3737e-02,  3.1615e-02, -2.2879e-01,\n",
      "        -1.6920e-02,  8.1124e-02, -1.7138e-01,  1.3196e-01, -1.6653e-01,\n",
      "        -4.8368e-02,  2.2461e-02, -5.2031e-02, -4.7352e-02,  2.9442e-02,\n",
      "         7.2490e-02, -7.2030e-03, -3.7497e-02,  2.5162e-02, -1.3300e-01,\n",
      "        -1.5405e-02, -5.1451e-02, -7.8030e-02, -6.0621e-03, -1.3564e-02,\n",
      "        -5.3518e-02, -2.3202e-02,  4.3213e-02,  5.9318e-02, -3.2409e-02,\n",
      "         7.1219e-02, -2.9018e-01, -2.9166e-01, -1.3476e-01, -4.5357e-02,\n",
      "         1.6238e-01, -1.7955e-02, -5.7692e-02,  1.1849e-01,  3.1481e-02,\n",
      "        -2.6842e-02,  1.6301e-01, -1.0856e-01,  1.1214e-01, -1.5431e-01,\n",
      "        -2.1536e-01, -1.9540e-01,  1.1765e-01,  5.4538e-02,  1.1171e-01,\n",
      "         1.1293e-01, -9.8502e-02, -1.1894e-01, -1.8609e-01, -9.6841e-02,\n",
      "        -1.1934e-01,  4.3758e-03, -2.7977e-04,  7.8381e-02, -1.5119e-01,\n",
      "         9.0544e-02, -1.5400e-01,  2.8042e-02,  2.0689e-02,  2.8785e-02,\n",
      "        -3.7832e-03, -1.3033e-01, -4.2850e-02, -3.4673e-01, -3.6412e-02,\n",
      "         3.6542e-02, -1.0957e-01,  1.8071e-01, -2.6734e-01,  1.4095e-01,\n",
      "        -1.7164e-01,  2.0624e-01, -2.8468e-01,  6.9966e-02,  4.2381e-02,\n",
      "        -1.3928e-01, -1.2000e-01,  5.8110e-02,  3.5377e-02,  2.0456e-01,\n",
      "        -3.5971e-02,  4.4443e-02, -6.7918e-05, -1.6254e-01,  1.3050e-02,\n",
      "         7.6801e-02,  6.1203e-02,  3.9709e-02, -7.1113e-03, -1.0502e-01,\n",
      "         6.8792e-04, -1.1825e-02, -2.0480e-02, -3.8859e-01, -1.8360e-01,\n",
      "        -2.1384e-02, -3.4778e-02,  6.3239e-02,  4.1039e-02, -1.6776e-01,\n",
      "         8.5346e-03, -9.7005e-02, -3.5047e-02, -1.9809e-02, -4.8108e-02,\n",
      "         9.9636e-02, -1.5308e-01,  4.2894e-02,  1.4008e-01, -5.3736e-02,\n",
      "         2.1166e-01, -1.4790e-01,  5.7403e-02,  8.2016e-03, -6.3173e-02,\n",
      "        -3.2982e-03, -1.1645e-02, -9.6557e-03,  1.0371e-01, -1.8417e-03,\n",
      "         7.7081e-02, -7.1388e-02, -7.2064e-03,  5.6943e-02, -9.5469e-03,\n",
      "        -7.7000e-02, -1.8005e-02,  4.7217e-02, -1.0395e-01, -1.6792e-03,\n",
      "         2.4720e-02, -1.1798e-01, -5.9704e-02, -5.6874e-02,  3.3214e-02])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 3297/14000 [00:02<00:08, 1209.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(X_test_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])):\n\u001b[1;32m     12\u001b[0m     contextualised_actions \u001b[38;5;241m=\u001b[39m mc_contextualiser\u001b[38;5;241m.\u001b[39mcontextualise(\n\u001b[1;32m     13\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(X_train_scaled[t], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0m     chosen_arm \u001b[38;5;241m=\u001b[39m \u001b[43mbandit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontextualised_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(chosen_arm)\n\u001b[1;32m     18\u001b[0m evaluate_model_direct(np\u001b[38;5;241m.\u001b[39marray(predictions), y_test)\n",
      "File \u001b[0;32m~/miniconda3/envs/neural_bandits/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/neural_bandits/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/neural-bandits/src/neural_bandits/algorithms/linear_bandits.py:24\u001b[0m, in \u001b[0;36mLinearTSBandit.forward\u001b[0;34m(self, contextualised_actions)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m contextualised_actions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_arms \u001b[38;5;129;01mand\u001b[39;00m contextualised_actions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContextualised actions must have shape (batch_size, n_arms, n_features)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m contextualised_actions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 24\u001b[0m theta_tilde \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mMultivariateNormal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msample((batch_size,))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39margmax(torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mijk,ik->ij\u001b[39m\u001b[38;5;124m\"\u001b[39m, contextualised_actions, theta_tilde), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# X_test, y_test\n",
    "def evaluate_model_direct(y_pred, y_test):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for t in tqdm.tqdm(range(X_test_scaled.shape[0])):\n",
    "    contextualised_actions = mc_contextualiser.contextualise(\n",
    "        torch.tensor(X_train_scaled[t], dtype=torch.float32).reshape(1, -1)\n",
    "    )\n",
    "    chosen_arm = bandit(contextualised_actions)\n",
    "    predictions.append(chosen_arm)\n",
    "\n",
    "evaluate_model_direct(np.array(predictions), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_bandits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
