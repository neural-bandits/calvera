# Notes 13.12.2024
- add epsilon greedy
- instead of RAG dataset: Learn to Rank dataset (i.e. only retrieval ranking)
- need parameters for certain algorithms (e.g. NeuralUCB because its n^2 to weights): list of strings which parts of the network are frozen (see LoRA style)? or search for trainable parameters
- NeuralLinear you want to define parts of module embedder and head 
- how to do bootstrap ?? try to do it similar to selecting nn experts => drop bootstrap
- drop LoRA
- add one exploration strategy (reasonable difficulty) OR more work 
- maybe don't need AbstractBandit
- focus on experiments: one NN, one Bandit, one ??
- Mini Tutorial in documentation
- think about lightning for good structure
- ArgMaxSelector, EpsilonGreedySelector, CombinatorialSelector, (SoftmaxSampling?)
- its ok not to cache the gradients so we have to do the forward pass through the network again
- trainer can be shared for (e.g. LinUCB and LinTS, but probably also NeuralUCB and NeuralTS)
- bandits should be built up from building blocks
- need to pass some hyperparameters to trainers
- Add environment: parameterized experiment runner (given bandit, given dataset, ...)
- Unit Tests: initialize with identity matrix, check that size of matrix is the same => catch runtime errors. Test edge cases (empty arrays etc.)
- NeuralLinearTrainer has two update methods